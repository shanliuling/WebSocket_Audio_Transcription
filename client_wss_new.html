<!DOCTYPE html>
<html>

<head>
    <meta charset="utf-8">
    <title>WebSocket Audio Transcription</title>
    <style>
        #transcriptionResult {
            white-space: pre-wrap;
            /* Preserve whitespace and line breaks */
            background-color: #f5f5f5;
            padding: 10px;
            border: 1px solid #ddd;
            border-radius: 5px;
            font-family: monospace;
        }

        #recordButton.recording {
            background-color: red;
            color: white;
        }
    </style>
</head>

<body>
    <!-- <script src="recorder-core.js" charset="UTF-8"></script> -->
    <div>
        <button id="recordButton">Start Recording</button>
        <label>
            language:
            <input id="lang" type="text" value="auto" />
        </label>
        <label>
            <input type="checkbox" id="speakerVerification"> Speaker Verification
        </label>
        <hr />
        Transcription result will be displayed below:
        <p id="transcriptionResult"></p>
    </div>
</body>
<script>
    var recordButton = document.getElementById('recordButton')
    var transcriptionResult = document.getElementById('transcriptionResult')
    navigator.getUserMedia = navigator.getUserMedia || navigator.webkitGetUserMedia
    var ws = null
    var record = null
    var timeInte = null
    var isRecording = false

    recordButton.onclick = function () {
        if (!isRecording) {
            startRecording()
        } else {
            stopRecording()
        }
    }

    function startRecording () {
        console.log('Start Recording')
        var speakerVerificationCheckbox = document.getElementById('speakerVerification')
        var sv = speakerVerificationCheckbox.checked ? 1 : 0
        var lang = document.getElementById("lang").value
        // Construct the query parameters
        var queryParams = []
        if (lang) {
            queryParams.push(`lang=${lang}`)
        }
        if (sv) {
            queryParams.push('sv=1')
        }
        var queryString = queryParams.length > 0 ? `?${queryParams.join('&')}` : ''

        ws = new WebSocket(`ws://100.64.0.22:27000/ws/transcribe${queryString}`)
        ws.binaryType = 'arraybuffer'

        ws.onopen = function (event) {
            console.log('WebSocket connection established')
            record.start()

            // 注释掉定时器发送，改为基于分贝的VAD控制发送
            console.log('使用基于分贝的VAD控制音频发送')
            /*
            timeInte = setInterval(function () {
                if (ws.readyState === 1) {
                    var audioBlob = record.getBlob()

                    // 只有当有音频数据时才发送
                    if (audioBlob.size > 0) {
                        console.log('Blob size: ', audioBlob.size)
                        ws.send(audioBlob)
                        console.log('Sending audio data, blob size:', audioBlob.size)
                        record.clear()
                        console.log('Buffer cleared')
                    }
                    record.clear()
                }
            }, 200)
            */
        }

        ws.onmessage = function (evt) {
            console.log('Received message: ' + evt.data)
            try {
                resJson = JSON.parse(evt.data)
                if (resJson.code == 0) {
                    var jsonResponse = JSON.stringify(resJson, null, 4)
                    transcriptionResult.textContent += "\n" + (resJson.data || 'No speech recognized')
                }
            } catch (e) {
                console.error('Failed to parse response data', e)
                transcriptionResult.textContent += "\n" + evt.data
            }
        }

        ws.onclose = function () {
            console.log('WebSocket connection closed')
        }

        ws.onerror = function (error) {
            console.error('WebSocket error: ' + error)
        }

        recordButton.textContent = "Stop Recording"
        recordButton.classList.add("recording")
        isRecording = true
    }

    function stopRecording () {
        console.log('Stop Recording')

        // 发送剩余的音频数据
        if (record) {
            var audioBlob = record.getBlob()
            if (audioBlob.size > 0 && ws && ws.readyState === 1) {
                console.log('停止录音时发送剩余音频数据，大小:', audioBlob.size)
                ws.send(audioBlob)
            }
        }

        if (ws) {
            ws.close()
            record.stop()
            // clearInterval(timeInte) // 不再使用定时器
        }
        recordButton.textContent = "Start Recording"
        recordButton.classList.remove("recording")
        isRecording = false
    }

    function init (rec) {
        record = rec
    }

    if (!navigator.getUserMedia) {
        alert('Your browser does not support audio input')
    } else {
        navigator.getUserMedia(
            {
                audio: {
                    noiseSuppression: true,  // 启用降噪
                    echoCancellation: true,  // 启用回声消除
                    autoGainControl: true,   // 启用自动增益控制
                    sampleRate: 16000
                }
            },
            function (mediaStream) {
                init(new Recorder(mediaStream))
            },
            function (error) {
                console.log(error)
            }
        )
    }

    var Recorder = function (stream) {
        // 基于分贝的语音检测相关变量
        var VAD_STATE = {
            SILENT: 0,
            VOICE: 1
        }

        // 分贝阈值设置
        var DECIBEL_THRESHOLD = -40 // 分贝阈值，可调整 (-60到-20之间)
        var MIN_VOICE_DURATION = 0.5 // 最小语音持续时间（秒）
        var MIN_SILENCE_DURATION = 1.0 // 最小静音持续时间（秒）
        var FRAME_DURATION = 4096 / 48000 // 每帧持续时间（秒）

        var currentState = VAD_STATE.SILENT
        var voiceDuration = 0 // 当前语音持续时间
        var silenceDuration = 0 // 当前静音持续时间
        var preSpeechBuffer = []
        var PRE_SPEECH_BUFFER_DURATION = 0.2 // 语音前保留0.2秒

        var sampleBits = 16 // Sample bits
        var inputSampleRate = 48000 // Input sample rate
        var outputSampleRate = 16000 // Output sample rate
        var channelCount = 1 // Single channel
        var context = new AudioContext()
        var audioInput = context.createMediaStreamSource(stream)
        var recorder = context.createScriptProcessor(4096, channelCount, channelCount)
        var audioData = {
            size: 0,
            buffer: [],
            inputSampleRate: inputSampleRate,
            inputSampleBits: sampleBits,
            clear: function () {
                this.buffer = []
                this.size = 0
            },
            input: function (data) {
                this.buffer.push(new Float32Array(data))
                this.size += data.length
            },
            encodePCM: function () {
                var bytes = new Float32Array(this.size)
                var offset = 0
                for (var i = 0; i < this.buffer.length; i++) {
                    bytes.set(this.buffer[i], offset)
                    offset += this.buffer[i].length
                }
                var dataLength = bytes.length * (sampleBits / 8)
                var buffer = new ArrayBuffer(dataLength)
                var data = new DataView(buffer)
                offset = 0
                for (var i = 0; i < bytes.length; i++, offset += 2) {
                    var s = Math.max(-1, Math.min(1, bytes[i]))
                    data.setInt16(offset, s < 0 ? s * 0x8000 : s * 0x7FFF, true)
                }
                return new Blob([data], { type: 'audio/pcm' })
            }
        }

        this.start = function () {
            audioInput.connect(recorder)
            recorder.connect(context.destination)
        }

        this.stop = function () {
            recorder.disconnect()
        }

        this.getBlob = function () {
            return audioData.encodePCM()
        }

        this.clear = function () {
            audioData.clear()
        }

        // 计算分贝值
        function calculateDecibels (buffer) {
            var sum = 0
            for (var i = 0; i < buffer.length; i++) {
                sum += buffer[i] * buffer[i]
            }
            var rms = Math.sqrt(sum / buffer.length)

            // 避免对0取对数
            if (rms < 0.000001) rms = 0.000001

            // 计算分贝值 (相对于满量程)
            var decibels = 20 * Math.log10(rms)
            return decibels
        }

        function downsampleBuffer (buffer, inputSampleRate, outputSampleRate) {
            if (outputSampleRate === inputSampleRate) {
                return buffer
            }
            var sampleRateRatio = inputSampleRate / outputSampleRate
            var newLength = Math.round(buffer.length / sampleRateRatio)
            var result = new Float32Array(newLength)
            var offsetResult = 0
            var offsetBuffer = 0
            while (offsetResult < result.length) {
                var nextOffsetBuffer = Math.round((offsetResult + 1) * sampleRateRatio)
                var accum = 0, count = 0
                for (var i = offsetBuffer; i < nextOffsetBuffer && i < buffer.length; i++) {
                    accum += buffer[i]
                    count++
                }
                result[offsetResult] = accum / count
                offsetResult++
                offsetBuffer = nextOffsetBuffer
            }
            return result
        }

        recorder.onaudioprocess = function (e) {
            var inputData = e.inputBuffer.getChannelData(0)

            // 1. 计算当前帧的分贝值
            var currentDecibels = calculateDecibels(inputData)

            // 2. 基于分贝的VAD状态机
            if (currentState === VAD_STATE.SILENT) {
                if (currentDecibels > DECIBEL_THRESHOLD) {
                    voiceDuration += FRAME_DURATION
                    silenceDuration = 0

                    if (voiceDuration >= MIN_VOICE_DURATION) {
                        console.log("检测到语音开始，分贝:", currentDecibels.toFixed(2), "dB，持续时间:", voiceDuration.toFixed(2), "秒")
                        currentState = VAD_STATE.VOICE

                        // 将缓冲区中的音频添加到主数据中（包含语音开始前的部分）
                        preSpeechBuffer.forEach(function (bufferedData) {
                            var resampled = downsampleBuffer(bufferedData, inputSampleRate, outputSampleRate)
                            audioData.input(resampled)
                        })
                        preSpeechBuffer = []
                    }
                } else {
                    voiceDuration = 0
                    silenceDuration += FRAME_DURATION
                }
            } else if (currentState === VAD_STATE.VOICE) {
                if (currentDecibels <= DECIBEL_THRESHOLD) {
                    silenceDuration += FRAME_DURATION
                    voiceDuration = 0

                    if (silenceDuration >= MIN_SILENCE_DURATION) {
                        console.log("检测到语音结束，分贝:", currentDecibels.toFixed(2), "dB，静音持续:", silenceDuration.toFixed(2), "秒")
                        currentState = VAD_STATE.SILENT

                        // 立即发送音频数据
                        if (audioData.size > 0 && ws && ws.readyState === 1) {
                            var audioBlob = audioData.encodePCM()
                            console.log("发送语音数据，大小:", audioBlob.size, "字节，时长:", (audioData.size / outputSampleRate).toFixed(2), "秒")
                            ws.send(audioBlob)
                            audioData.clear()
                        }

                        voiceDuration = 0
                        silenceDuration = 0
                    }
                } else {
                    silenceDuration = 0
                    voiceDuration += FRAME_DURATION
                }
            }

            // 3. 根据状态处理音频
            if (currentState === VAD_STATE.VOICE) {
                // 正在说话，录制音频
                var resampledData = downsampleBuffer(inputData, inputSampleRate, outputSampleRate)
                audioData.input(resampledData)
            } else {
                // 静音状态，保留少量音频到缓冲区
                preSpeechBuffer.push(new Float32Array(inputData))

                // 保持缓冲区在指定时长内
                var maxBufferFrames = Math.ceil(PRE_SPEECH_BUFFER_DURATION / FRAME_DURATION)
                if (preSpeechBuffer.length > maxBufferFrames) {
                    preSpeechBuffer.shift()
                }
            }

            // 调试信息（可选，每秒打印一次）
            if (Math.random() < 0.02) { // 约每秒打印一次
                console.log("实时分贝:", currentDecibels.toFixed(2), "dB，状态:",
                    currentState === VAD_STATE.VOICE ? "语音" : "静音",
                    "，语音时长:", voiceDuration.toFixed(2), "秒")
            }
        }
    };

</script>

</html>